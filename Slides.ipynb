{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "603e5f2f",
   "metadata": {},
   "source": [
    "# MAPDb - Final Project\n",
    "\n",
    "## Streaming processing of cosmic rays using Drift Tubes detectors\n",
    "\n",
    "The goal of this project is to reproduce a real-time processing of real data collected in a particle physics detector and publish the results in a dashboard for live monitoring.\n",
    "\n",
    "### Students:\n",
    "* Conforto Filippo (2021856)\n",
    "* Domenichetti Lorenzo (2011653)\n",
    "* Faorlin Tommaso (2021857)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b919b72e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736dd411",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"cluster.png\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b7116b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "After ```cp spark-defaults.conf.template spark-defaults.conf``` we modify this file adding the following lines:\n",
    "\n",
    "```\n",
    "spark.executor.memory          1800m\n",
    "spark.executor.instances          15\n",
    "spark.executor.cores               1\n",
    "```\n",
    "In the end we will instantiate in total **15 executors** (3 for each Worker node) with **one cores each** and **1,8 GB** of RAM. We also tried with 20 executors and two cores each and in th end we obtain more or less the same preformances in the long run (after the initial 'stabilization phase'). We chose in the end the first configuration to be sure we are leaving one core and 1 GB of RAM free for other VMs operations. \n",
    "\n",
    "We notice that leaving one core free allows to have a more stable input rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7c80f8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#initialisation of spark from the packages folder\n",
    "findspark.init('/usr/local/spark')\n",
    "\n",
    "#start session - specify port, application name, and configuration settings\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"spark://master:7077\")\\\n",
    "    .appName(\"MAPD Final Project session\")\\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\")\\\n",
    "    .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "#default parallelism setting to shuffle different partitions between workers\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f5a1490",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MAPD Final Project session</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fee982ec320>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check whether the session is running.\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ff611",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kafka server setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673ec314",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```/home/packages/kafka_2.13-2.7.0/bin/zookeeper-server-start.sh /home/packages/kafka_2.13-2.7.0/config/zookeeper.properties```\n",
    "\n",
    "```/home/packages/kafka_2.13-2.7.0/bin/kafka-server-start.sh /home/packages/kafka_2.13-2.7.0/config/server.properties ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357751ba",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#define the kafka server from IP and Port\n",
    "KAFKA_BOOTSTRAP_SERVERS = \"slave04:9092\" #on the fourth VM \n",
    "\n",
    "#define the input dataframe and its source. Define subscription to 'topic_stream'\n",
    "inputDF = spark\\\n",
    "        .readStream\\\n",
    "        .format(\"kafka\")\\\n",
    "        .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS)\\\n",
    "        .option('subscribe', 'topic_stream')\\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee98a2e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448d6ca0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#SSL context to dowload without errors data from the given server\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "#define the kafka server from IP and Port\n",
    "KAFKA_BOOTSTRAP_SERVERS = 'slave04:9092'\n",
    "\n",
    "#producer definition from IP address given before\n",
    "producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)\n",
    "\n",
    "\n",
    "for i in range(0, 81):\n",
    "    #data download from s3 bucket\n",
    "    i = str(i).zfill(2)\n",
    "    url = f\"https://cloud-areapd.pd.infn.it:5210/swift/v1/\\\n",
    "            AUTH_d2e941ce4b324467b6b3d467a923a9bc/MAPD_miniDT_stream/data_0000{i}.txt\"\n",
    "    df = pd.read_csv(url)\n",
    "    \n",
    "    #data cleaning for possible outliers\n",
    "    df = df[df.ORBIT_CNT < 5e8]\n",
    "    print(f\"Reading file data_0000{i}.txt\")\n",
    "    \n",
    "    #for loop over file size\n",
    "    for j in tqdm(range(0, df.shape[0])):\n",
    "        \n",
    "        #dictionaries creation from dataframe's rows\n",
    "        jj = df.iloc[j].to_dict()\n",
    "        \n",
    "        #unnecessary floats are cast into ints\n",
    "        for key in ['HEAD', 'FPGA', \"TDC_CHANNEL\"]:\n",
    "            jj[key] = int(jj[key])\n",
    "            \n",
    "        #json row is sent to the Kafka topic 'topic_stream'\n",
    "        producer.send('topic_stream',\n",
    "                      json.dumps(jj).encode('utf-8')\n",
    "                     )\n",
    "        time.sleep(0.0003)\n",
    "        \n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a13edc4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55e5a8d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37d5aac",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#define the schema of the rows that will be read\n",
    "#double are used to overcome overflow issues\n",
    "schema = StructType(\n",
    "        [StructField(\"HEAD\",        IntegerType()),\n",
    "         StructField(\"FPGA\",         IntegerType()),\n",
    "         StructField(\"TDC_CHANNEL\",  IntegerType()),\n",
    "         StructField(\"ORBIT_CNT\",    DoubleType()),\n",
    "         StructField(\"BX_COUNTER\",   DoubleType()), \n",
    "         StructField(\"TDC_MEAS\",    DoubleType() )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399d6769",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#convert input_Df to json by casting columns into the predefined schema.\n",
    "jsonDF = inputDF.select(from_json(col(\"value\").alias('value').cast(\"string\"),\n",
    "                                  schema).alias('value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d3600a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#flattening the dataframe\n",
    "flatDF = jsonDF.selectExpr(\"value.HEAD\", \n",
    "                           \"value.FPGA\", \n",
    "                           \"value.TDC_CHANNEL\",\n",
    "                           \"value.ORBIT_CNT\",\n",
    "                           \"value.BX_COUNTER\",\n",
    "                           \"value.TDC_MEAS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8893c506",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean dataframe, removing ancillary hits\n",
    "df = flatDF.where(col(\"HEAD\")==2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb9aff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scintillator time offset by Chamber\n",
    "time_offset_by_chamber = {\n",
    "0: 95.0 - 1.1, # Ch 0\n",
    "1: 95.0 + 6.4, # Ch 1\n",
    "2: 95.0 + 0.5, # Ch 2\n",
    "3: 95.0 - 2.6, # Ch 3\n",
    "}\n",
    "\n",
    "#bins definition for histograms - they will be shared among all iterations.\n",
    "binning = list(linspace(0, 4e8, 30))\n",
    "\n",
    "binning_drift = list(linspace(0, 800, 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c138d859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_proc(batch_df, epoch_id):\n",
    "    \n",
    "    #repartition the df DataFrame to 100 parts\n",
    "    #and persist in cache to speedup calculations\n",
    "    batch_df.coalesce(100)\n",
    "    batch_df.persist()\n",
    "    \n",
    "    #Dividing the dataframe between chambers\n",
    "    batch_df_ch0 = batch_df.filter('(FPGA==0) AND (TDC_CHANNEL >= 0) AND (TDC_CHANNEL < 64)')\n",
    "    batch_df_ch1 = batch_df.filter('(FPGA==0) AND (TDC_CHANNEL >= 64) AND (TDC_CHANNEL < 128)')\n",
    "    batch_df_ch2 = batch_df.filter('(FPGA==1) AND (TDC_CHANNEL >= 0) AND (TDC_CHANNEL < 64)')\n",
    "    batch_df_ch3 = batch_df.filter('(FPGA==1) AND (TDC_CHANNEL >= 64) AND (TDC_CHANNEL < 128)')\n",
    "    \n",
    "    #coalesce + persist for each filtered batch\n",
    "    batch_df_ch0.coalesce(100)\n",
    "    batch_df_ch1.coalesce(100)\n",
    "    batch_df_ch2.coalesce(100)\n",
    "    batch_df_ch3.coalesce(100)\n",
    "    batch_df_ch0.persist()\n",
    "    batch_df_ch1.persist()\n",
    "    batch_df_ch2.persist()\n",
    "    batch_df_ch3.persist()    \n",
    "       \n",
    "    #list of dfs_ handy for loops\n",
    "    batch_dfs = [batch_df_ch0, batch_df_ch1, batch_df_ch2, batch_df_ch3]\n",
    "    \n",
    "    #counting hits for each chamber\n",
    "    hits_ch0 = batch_df_ch0.count()\n",
    "    hits_ch1 = batch_df_ch1.count()\n",
    "    hits_ch2 = batch_df_ch2.count()\n",
    "    hits_ch3 = batch_df_ch3.count()\n",
    "    \n",
    "    #total number of hits as the sum of the previous 4 operations\n",
    "    hits = hits_ch0 + hits_ch1 + hits_ch2 + hits_ch3\n",
    "    \n",
    "    if hits!=0: \n",
    "\n",
    "        #dataframe containing only informations about scintillator events\n",
    "        batch_df_scint = (batch_df.filter('(FPGA==1) AND (TDC_CHANNEL == 128)')\n",
    "                          .select(['ORBIT_CNT', 'BX_COUNTER',\"TDC_MEAS\"])\n",
    "                          .groupBy('ORBIT_CNT')\n",
    "                          .min()\n",
    "                          .withColumnRenamed(\"min(BX_COUNTER)\", 'BX_COUNTER_SCINT')\n",
    "                          .withColumnRenamed(\"min(TDC_MEAS)\", 'TDC_MEAS_SCINT')\n",
    "                          .drop('min(ORBIT_CNT)')\n",
    "                         )\n",
    "\n",
    "        #total active channels histogram\n",
    "        hist1 = {}\n",
    "        \n",
    "        #channels per orbit histogram \n",
    "        hist2 = {}\n",
    "\n",
    "        #active channels in orbits in which the scintillator is active histogram\n",
    "        hist3 = {}\n",
    "\n",
    "        #drifttime histogram\n",
    "        hist4 = {}\n",
    "        \n",
    "        #loop over the four chambers\n",
    "        for chamber in [0,1,2,3]:\n",
    "            #create an empty dictionary for each type of histogram corresponding to the selected chamber\n",
    "            hist1[chamber] = {}\n",
    "            hist2[chamber] = {}\n",
    "            hist3[chamber] = {}\n",
    "            hist4[chamber] = {}\n",
    "\n",
    "            #TDC_channel simple histogram - adaptive binning depending on the corresponding channels\n",
    "            bins, counts = (\n",
    "                batch_dfs[chamber].select('TDC_CHANNEL')\n",
    "                .rdd.map(lambda x: x.TDC_CHANNEL)\n",
    "                .histogram(list(arange((chamber % 2)*64,(chamber % 2 +1)*64,1)))\n",
    "            )\n",
    "            \n",
    "            #count number of orbits per TDC_CHANNEL\n",
    "            bins2, counts2 = (\n",
    "                batch_dfs[chamber].groupBy(\"ORBIT_CNT\",\"TDC_CHANNEL\").count()\n",
    "                .select('ORBIT_CNT')\n",
    "                .rdd.map(lambda x: x.ORBIT_CNT)\n",
    "                .histogram(binning)\n",
    "            )\n",
    "            \n",
    "            #filtering only useful hits (avoid scintillators) and use inner join to consider coincident events\n",
    "            batch_dfs[chamber] = batch_dfs[chamber].filter('NOT ((FPGA==1) AND (TDC_CHANNEL == 128))')\\\n",
    "                                                   .join(batch_df_scint, [\"ORBIT_CNT\"], \"inner\")\n",
    "            #creating driftime and select only positive values\n",
    "            #values smaller than zero should be artifacts - only a few percentage\n",
    "            batch_dfs[chamber] = batch_dfs[chamber].withColumn('DRIFTIME', \n",
    "                                                               25*((col('BX_COUNTER')-\\\n",
    "                                                                    col('BX_COUNTER_SCINT'))+\\\n",
    "                                                                   (col('TDC_MEAS')-\\\n",
    "                                                                    col('TDC_MEAS_SCINT'))/30)+\\\n",
    "                                                               time_offset_by_chamber[chamber])\\\n",
    "                                                   .where(col('DRIFTIME')>0) \n",
    "            \n",
    "            #TDC_CHANNEL histogram after scintillator selection\n",
    "            bins3, counts3 = (\n",
    "                batch_dfs[chamber].select('TDC_CHANNEL')\n",
    "                .rdd.map(lambda x: x.TDC_CHANNEL)\n",
    "                .histogram(list(arange((chamber % 2)*64,(chamber % 2 +1)*64,1)))\n",
    "            )\n",
    "\n",
    "            #histogram for drifttime - a \"box\" is expected\n",
    "            bins4, counts4 = (\n",
    "                batch_dfs[chamber].select('DRIFTIME')\n",
    "                .rdd.map(lambda x: x.DRIFTIME)\n",
    "                .histogram(binning_drift)\n",
    "            )\n",
    "            \n",
    "            \n",
    "            #convert to python integers both bins and counts\n",
    "            hist1[chamber]['bins'] = list(map(int,bins)) \n",
    "            hist1[chamber]['counts'] = list(map(int,counts))\n",
    "\n",
    "            hist2[chamber]['bins'] = list(map(int,bins2))\n",
    "            hist2[chamber]['counts'] = list(map(int,counts2))\n",
    "\n",
    "            hist3[chamber]['bins'] = list(map(int,bins3))\n",
    "            hist3[chamber]['counts'] = list(map(int,counts3))\n",
    "\n",
    "            hist4[chamber]['bins'] = list(map(int,bins4))\n",
    "            hist4[chamber]['counts'] = list(map(int,counts4))\n",
    "\n",
    "        #producing the results dictionary\n",
    "        result = {\n",
    "            \"hits\" : hits,\n",
    "            \"hits_per_chamber\": [hits_ch0, hits_ch1, hits_ch2, hits_ch3],\n",
    "            \"hist_1\": hist1,\n",
    "            \"hist_2\": hist2,\n",
    "            \"hist_3\": hist3,\n",
    "            \"hist_4\": hist4\n",
    "        }\n",
    "\n",
    "        #sending the json to the producer\n",
    "        producer.send('topic_results', json.dumps(result).encode('utf-8'))\n",
    "\n",
    "        #unpersist DataFrames and free resources\n",
    "        batch_df.unpersist()\n",
    "        batch_df_ch0.unpersist()\n",
    "        batch_df_ch1.unpersist()\n",
    "        batch_df_ch2.unpersist()\n",
    "        batch_df_ch3.unpersist() \n",
    "        \n",
    "    else: \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcb92dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#producer definition from IP address given before\n",
    "producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)\n",
    "\n",
    "#process each batch as a WriteStream - 5 seconds batch - rate 1kHz \n",
    "df.writeStream\\\n",
    "    .foreachBatch(batch_proc)\\\n",
    "    .trigger(processingTime='5 second')\\\n",
    "    .start()\\\n",
    "    .awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc1431f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e40a01",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#define the kafka server from IP and Port\n",
    "KAFKA_BOOTSTRAP_SERVERS='slave04:9092'\n",
    "\n",
    "#consumer definition from IP address given before\n",
    "consumer = KafkaConsumer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS, consumer_timeout_ms=100000)\n",
    "\n",
    "#consumer subscription to topic_results\n",
    "consumer.subscribe('topic_results')\n",
    "\n",
    "#Additional configuration options for consumer\n",
    "consumer.poll(timeout_ms=0,         # do not enable dead-times before one poll to the next\n",
    "              max_records=None,     # do not limit the number of records to consume at once \n",
    "              update_offsets=True   # update the reading offsets on this topic\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bb1178",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def rt_plot(js,num, fig,axes):\n",
    "    \n",
    "    #to delete the texts written during the previous update\n",
    "    for txt in fig.texts:\n",
    "        txt.set_visible(False)\n",
    "    \n",
    "    #label with informations on the right side of the window\n",
    "    plt.figtext(1.01, 0.5,\n",
    "                f'Batch N° {num}:\\n\\nHits: {js[\"hits\"]} \\n\\n' +\\\n",
    "                f'Channel 0: {js[\"hits_per_chamber\"][0]} hits\\n\\n' +\\\n",
    "                f'Channel 1: {js[\"hits_per_chamber\"][1]} hits\\n\\n' +\\\n",
    "                f'Channel 2: {js[\"hits_per_chamber\"][2]} hits\\n\\n' +\\\n",
    "                f'Channel 3: {js[\"hits_per_chamber\"][3]} hits' \n",
    "                , ha='left', va='center', fontsize = 20, \n",
    "                bbox=dict(facecolor='snow',\n",
    "                          edgecolor='black',\n",
    "                          boxstyle='round'))\n",
    "    \n",
    "    #main title of the upper panel\n",
    "    plt.figtext(0.5, 1.07,\n",
    "                f'Muon-Hits Monitors',\n",
    "                ha='center', va='center', fontsize = 30)\n",
    "    plt.figtext(0.5, 1.025,\n",
    "                f'Total active channels',\n",
    "                ha='center', va='center', fontsize = 22)\n",
    "                \n",
    "    for i, ax in enumerate(axes[0]):\n",
    "        #clean previous plots and update titles and labels\n",
    "        ax.clear()\n",
    "        ax.set_title(f'Chamber {i}', fontsize=15)\n",
    "        ax.set_xlabel(\"Channel N°\",  fontsize=14)\n",
    "        if i==0:\n",
    "            ax.set_ylabel(\"Counts\",  fontsize=14)\n",
    "        #bins centers computation\n",
    "        bin_centers = js[\"hist_1\"][str(i)][\"bins\"][:-1] +\\\n",
    "                np.diff(js[\"hist_1\"][str(i)][\"bins\"])/2\n",
    "        ax.hist(bin_centers,\n",
    "                weights=js[\"hist_1\"][str(i)][\"counts\"],\n",
    "                bins=js[\"hist_1\"][str(i)][\"bins\"],\n",
    "                alpha=0.6)\n",
    "        ax.tick_params(labelsize=13)\n",
    "\n",
    "    \n",
    "    #main title of the second panel\n",
    "    plt.figtext(0.5, 0.75,\n",
    "                'Total number of active channels per orbit',\n",
    "                ha='center', va='center', fontsize = 22)\n",
    "\n",
    "    for i, ax in enumerate(axes[1]):\n",
    "        #clean previous plots and update titles and labels\n",
    "        ax.clear()\n",
    "        ax.set_title(f'Chamber {i}', fontsize=15)\n",
    "        ax.set_xlabel(\"Orbit N°\",    fontsize=14)\n",
    "        if i==0:\n",
    "            ax.set_ylabel(\"Counts\",  fontsize=14)\n",
    "        #bins centers computation\n",
    "        bin_centers = js[\"hist_2\"][str(i)][\"bins\"][:-1] +\\\n",
    "                    np.diff(js[\"hist_2\"][str(i)][\"bins\"])/2\n",
    "        ax.hist(bin_centers,\n",
    "                weights=js[\"hist_2\"][str(i)][\"counts\"],\n",
    "                bins=js[\"hist_2\"][str(i)][\"bins\"],\n",
    "                alpha=0.6)\n",
    "        ax.tick_params(labelsize=13)\n",
    "        #scientific notation on the x-axis labels\n",
    "        ax.ticklabel_format(axis=\"x\", style=\"sci\", scilimits=(0,0)) \n",
    "        \n",
    "    #main title of the third panel\n",
    "    plt.figtext(0.5, 0.5,\n",
    "                f'Total active channels for orbits with active scintillator',\n",
    "                ha='center', va='center', fontsize = 22)\n",
    "                \n",
    "    for i, ax in enumerate(axes[2]):\n",
    "        #clean previous plots and update titles and labels\n",
    "        ax.clear()\n",
    "        ax.set_title(f'Chamber {i}', fontsize=15)\n",
    "        ax.set_xlabel(\"Channel N°\",  fontsize=14)\n",
    "        if i==0:\n",
    "            ax.set_ylabel(\"Counts\",  fontsize=14)\n",
    "        #bins centers computation\n",
    "        bin_centers = js[\"hist_3\"][str(i)][\"bins\"][:-1] +\\\n",
    "                    np.diff(js[\"hist_3\"][str(i)][\"bins\"])/2\n",
    "        ax.hist(bin_centers,\n",
    "                weights=js[\"hist_3\"][str(i)][\"counts\"],\n",
    "                bins=js[\"hist_3\"][str(i)][\"bins\"],\n",
    "                alpha=0.6)\n",
    "        ax.tick_params(labelsize=13)\n",
    "\n",
    "    \n",
    "    #main title of the lower panel\n",
    "    plt.figtext(0.5, 0.24,\n",
    "                'Drift time',\n",
    "                ha='center', va='center', fontsize = 22)\n",
    "\n",
    "    for i, ax in enumerate(axes[3]):\n",
    "        #clean previous plots and update titles and labels\n",
    "        ax.clear()\n",
    "        ax.set_title(f'Chamber {i}', fontsize=15)\n",
    "        ax.set_xlabel(\"Time[s]\",     fontsize=14)\n",
    "        if i==0:\n",
    "            ax.set_ylabel(\"Counts\",  fontsize=14)\n",
    "        #bins centers computation\n",
    "        bin_centers = js[\"hist_4\"][str(i)][\"bins\"][:-1] +\\\n",
    "                    np.diff(js[\"hist_4\"][str(i)][\"bins\"])/2\n",
    "        ax.hist(bin_centers,\n",
    "                weights=js[\"hist_4\"][str(i)][\"counts\"],\n",
    "                bins=js[\"hist_4\"][str(i)][\"bins\"], alpha=0.6)\n",
    "        ax.tick_params(labelsize=13)\n",
    "        #scientific notation on the x-axis labels\n",
    "        ax.ticklabel_format(axis=\"x\", style=\"sci\", scilimits=(0,0)) \n",
    "     \n",
    "    #to separate better the plots\n",
    "    plt.tight_layout(h_pad = 7, w_pad = 1)\n",
    "    \n",
    "    #clean the whole screen and update with new incoming data\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(pl.gcf())\n",
    "\n",
    "def bin_sum(old_j,new_j):\n",
    "    #updating old values \n",
    "    for i,name in enumerate([\"hits\",\n",
    "                             \"hits_per_chamber\",\n",
    "                             \"hist_1\",\n",
    "                             \"hist_2\",\n",
    "                             \"hist_3\",\n",
    "                             \"hist_4\"]):\n",
    "            if i==0:\n",
    "                #to add new hits\n",
    "                old_j[name]+=new_j[name]\n",
    "            elif i==1:\n",
    "                #to add new hits per chamber\n",
    "                old_j[name]=np.asarray(old_j[name]) +\\\n",
    "                np.asarray(new_j[name])\n",
    "            else:\n",
    "                for j in range(4):\n",
    "                    #to update counts with the incoming data array\n",
    "                    old_j[name][str(j)][\"counts\"]=np.asarray(old_j[name][str(j)][\"counts\"]) +\\\n",
    "                                                  np.asarray(new_j[name][str(j)][\"counts\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73e00dc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cumulative dashboard service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cba483c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Figure creation\n",
    "fig, axes = plt.subplots(4,4, figsize = (15,20), sharey = 'row')\n",
    "for num,message in enumerate(consumer):\n",
    "    if num==0:\n",
    "        #The first incoming message is simply stored\n",
    "        msg = json.loads(message.value)\n",
    "    else: \n",
    "        #The messages following are obtained as an update of the first one\n",
    "        bin_sum(msg,json.loads(message.value))\n",
    "    #plot production\n",
    "    rt_plot(msg,num,fig,axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9bbf63",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"graphs.png\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51c14b0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Single batch analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae339a8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Figure creation\n",
    "fig, axes = plt.subplots(4,4, figsize = (15,20), sharey = 'row')\n",
    "for num,message in enumerate(consumer):\n",
    "    #Creating and plotting values for the received message\n",
    "    msg = json.loads(message.value)\n",
    "    rt_plot(msg,num,fig,axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2313a39",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"graph2.png\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdbab64",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Performance analysis on the WebUI (localhost:4040)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b77d0f7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"webui_perfromances.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "**Process rate**\n",
    "\n",
    "We notice an initial delay in the Spark application processing.\n",
    "\n",
    "**Batch Duration**\n",
    "\n",
    "After an initial transient the processing time stabilizes below 5s. Sometimes, the batch processing times has some spikes and this is due to the fact that Executors die sometimes and they quickly restore afterwards. The choice to use small Executors prevents this spikes to be higher and so problematic.\n",
    "\n",
    "<br><br><br><br>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
