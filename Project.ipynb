{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Management and Analysis of Physics Dataset - mod.B\n",
    "\n",
    "## Final project: Streaming processing of cosmic rays using Drift Tubes detectors\n",
    "\n",
    "The goal of this project is to reproduce a real-time processing of real data collected in a particle physics detector and publish the results in a dashboard for live monitoring.\n",
    "\n",
    "### Students:\n",
    "* Conforto Filippo (2021856)\n",
    "* Domenichetti Lorenzo (2011653)\n",
    "* Faorlin Tommaso (2021857)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Streaming notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "import findspark\n",
    "import numpy as np\n",
    "from numpy import arange\n",
    "from numpy import linspace \n",
    "\n",
    "from kafka import KafkaProducer\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import from_json, col, max, min\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Creating Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('/home/packages/spark-3.1.2-bin-hadoop3.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .master(\"spark://10.67.22.100:7077\")\\\n",
    "    .appName(\"MAPD Final Project session\")\\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\")\\\n",
    "    .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://10.67.22.100:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MAPD Final Project session</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff7a3bb6d68>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_BOOTSTRAP_SERVERS = \"10.67.22.226:9092\" #:226 corresponds to slave 04. \n",
    "\n",
    "inputDF = spark\\\n",
    "    .readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS)\\\n",
    "    .option('subscribe', 'topic_stream')\\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "        [StructField(\"HEAD\",        IntegerType()),\n",
    "         StructField(\"FPGA\",         IntegerType()),\n",
    "         StructField(\"TDC_CHANNEL\",  IntegerType()),\n",
    "         StructField(\"ORBIT_CNT\",    DoubleType()), ## Double to overcome overflow problems\n",
    "         StructField(\"BX_COUNTER\",   DoubleType()), ## Double to overcome overflow problems\n",
    "         StructField(\"TDC_MEAS\",    DoubleType() )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonDF = inputDF.select(from_json(col(\"value\").alias('value').cast(\"string\"), schema).alias('value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten out the dataframe\n",
    "flatDF = jsonDF.selectExpr(\"value.HEAD\", \n",
    "                           \"value.FPGA\", \n",
    "                           \"value.TDC_CHANNEL\",\n",
    "                           \"value.ORBIT_CNT\",\n",
    "                           \"value.BX_COUNTER\",\n",
    "                           \"value.TDC_MEAS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Cleaning the upcoming dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = flatDF.where(col(\"HEAD\")==2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading to consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scintillator time offset by Chamber\n",
    "time_offset_by_chamber = {\n",
    "0: 95.0 - 1.1, # Ch 0\n",
    "1: 95.0 + 6.4, # Ch 1\n",
    "2: 95.0 + 0.5, # Ch 2\n",
    "3: 95.0 - 2.6, # Ch 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_proc(batch_df, epoch_id):\n",
    "    \n",
    "    hits = batch_df.count()\n",
    "    \n",
    "    if hits!=0: \n",
    "        start = time.time()\n",
    "        #Creating the ABSOLUTETIME column\n",
    "        batch_df = batch_df.withColumn(\"ABSOLUTETIME\", 25*(col('ORBIT_CNT')*1e-9*3564+col('BX_COUNTER')*1e-9+(col('TDC_MEAS')*1e-9)/30))\n",
    "        \n",
    "        #Dataframe containing only informations about scintillator events\n",
    "        batch_df_scint = (batch_df.filter('(FPGA==1) AND (TDC_CHANNEL == 128)')\n",
    "                        .select(['ORBIT_CNT', 'ABSOLUTETIME'])\n",
    "                        .groupBy('ORBIT_CNT')\n",
    "                        .min()\n",
    "                        .withColumnRenamed(\"min(ABSOLUTETIME)\", 'ABSOLUTETIME_SCINT')\n",
    "                        .drop('min(ORBIT_CNT)')\n",
    "                       )\n",
    "        #Dividing the dataframe between chambers\n",
    "        \n",
    "        batch_df_ch0 = batch_df.filter('(FPGA==0) AND (TDC_CHANNEL >= 0) AND (TDC_CHANNEL < 64)')\n",
    "        batch_df_ch1 = batch_df.filter('(FPGA==0) AND (TDC_CHANNEL >= 64) AND (TDC_CHANNEL < 128)')\n",
    "        batch_df_ch2 = batch_df.filter('(FPGA==1) AND (TDC_CHANNEL >= 0) AND (TDC_CHANNEL < 64)')\n",
    "        batch_df_ch3 = batch_df.filter('(FPGA==1) AND (TDC_CHANNEL >= 64) AND (TDC_CHANNEL < 128)')\n",
    "        \n",
    "        batch_dfs = [batch_df_ch0, batch_df_ch1, batch_df_ch2, batch_df_ch3]\n",
    "        \n",
    "        # Counting hits for each chamber\n",
    "\n",
    "        hits_ch0 = batch_df_ch0.count()\n",
    "        hits_ch1 = batch_df_ch1.count()\n",
    "        hits_ch2 = batch_df_ch2.count()\n",
    "        hits_ch3 = batch_df_ch3.count()\n",
    "\n",
    "        end = time.time()-start\n",
    "        #Total active channels histogram\n",
    "        \n",
    "        hist1 = {}\n",
    "        \n",
    "        #Channels per orbit histogram\n",
    "            \n",
    "        hist2 = {}\n",
    "        min_hist, max_hist = batch_df.agg(min(\"ORBIT_CNT\"), max(\"ORBIT_CNT\")).rdd.flatMap(lambda x: x).collect()\n",
    "        binning = list(linspace(0, 4e8, 30))\n",
    "\n",
    "        #Active channels in orbits in which the scintillator is active histogram\n",
    "        hist3 = {}\n",
    "\n",
    "        #Drifttime histogram\n",
    "        hist4 = {}\n",
    "        #Binning for driftime - arbitrario perché a volte non troviamo né max né min.. da rivedere.\n",
    "        binning_drift = list(linspace(0,800, 40))\n",
    "        \n",
    "        \n",
    "        \n",
    "        for chamber in [0,1,2,3]:\n",
    "            hist1[chamber] = {}\n",
    "            hist2[chamber] = {}\n",
    "            hist3[chamber] = {}\n",
    "            hist4[chamber] = {}\n",
    "\n",
    "            bins, counts = (\n",
    "                batch_dfs[chamber].select('TDC_CHANNEL')\n",
    "                .rdd.map(lambda x: x.TDC_CHANNEL)\n",
    "                .histogram(list(arange((chamber % 2)*64,(chamber % 2 +1)*64,1)))\n",
    "            )\n",
    "            \n",
    "            bins2, counts2 = (\n",
    "                batch_dfs[chamber].groupBy(\"ORBIT_CNT\",\"TDC_CHANNEL\").count()\n",
    "                .select('ORBIT_CNT')\n",
    "                .rdd.map(lambda x: x.ORBIT_CNT)\n",
    "                .histogram(binning)\n",
    "            )\n",
    "            \n",
    "            #Filtering only useful hits (avoid scintillators) and use inner join to consider coincident events\n",
    "            batch_dfs[chamber] = batch_dfs[chamber].filter('NOT ((FPGA==1) AND (TDC_CHANNEL == 128))')\\\n",
    "                                                   .join(batch_df_scint, [\"ORBIT_CNT\"], \"inner\")\n",
    "                \n",
    "            #Creating driftime and select only positive values\n",
    "            batch_dfs[chamber] = batch_dfs[chamber].withColumn('DRIFTIME', (col('ABSOLUTETIME')-col('ABSOLUTETIME_SCINT'))*1e9+time_offset_by_chamber[chamber])\\\n",
    "                                .where(col('DRIFTIME')>0) #Is it possible to remove it?\n",
    "            \n",
    "            bins3, counts3 = (\n",
    "                batch_dfs[chamber].select('TDC_CHANNEL')\n",
    "                .rdd.map(lambda x: x.TDC_CHANNEL)\n",
    "                .histogram(list(arange((chamber % 2)*64,(chamber % 2 +1)*64,1)))\n",
    "            )\n",
    "\n",
    "            bins4, counts4 = (\n",
    "                batch_dfs[chamber].select('DRIFTIME')\n",
    "                .rdd.map(lambda x: x.DRIFTIME)\n",
    "                .histogram(binning_drift)\n",
    "            )\n",
    "\n",
    "            hist1[chamber]['bins'] = list(map(int,bins)) #must convert to python integers\n",
    "            hist1[chamber]['counts'] = list(map(int,counts))\n",
    "        \n",
    "            hist2[chamber]['bins'] = list(map(int,bins2))\n",
    "            hist2[chamber]['counts'] = list(map(int,counts2))\n",
    "            \n",
    "            hist3[chamber]['bins'] = list(map(int,bins3))\n",
    "            hist3[chamber]['counts'] = list(map(int,counts3))\n",
    "        \n",
    "            hist4[chamber]['bins'] = list(map(int,bins4))\n",
    "            hist4[chamber]['counts'] = list(map(int,counts4))\n",
    "\n",
    "        #Producing the results dictionary\n",
    "        result = {\n",
    "            \"hits\" : hits,\n",
    "            \"hits_per_chamber\": [hits_ch0, hits_ch1, hits_ch2, hits_ch3],\n",
    "            \"hist_1\": hist1,\n",
    "            \"hist_2\": hist2,\n",
    "            \"hist_3\": hist3,\n",
    "            \"hist_4\": hist4\n",
    "        }\n",
    "        \n",
    "        #Sending the json to the producer\n",
    "        producer.send('topic_results', json.dumps(result).encode('utf-8'))\n",
    "#         producer.flush()\n",
    "        \n",
    "    else: \n",
    "        pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5440440d9318>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mtrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessingTime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'5 second'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mforeachBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_proc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df.writeStream\\\n",
    "    .trigger(processingTime='5 second')\\\n",
    "    .foreachBatch(batch_proc)\\\n",
    "    .start()\\\n",
    "    .awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset on dataset/lecture2/dimuon\n",
    "\n",
    "schema = StructType()                          \\\n",
    "      .add(\"HEAD\",        IntegerType(), True) \\\n",
    "      .add(\"FPGA\",        IntegerType(), True) \\\n",
    "      .add(\"TDC_CHANNEL\", IntegerType(), True) \\\n",
    "      .add(\"ORBIT_CNT\",   IntegerType(), True) \\\n",
    "      .add(\"BX_COUNTER\",  IntegerType(), True) \\\n",
    "      .add(\"TDC_MEAS\",    DoubleType(),  True)\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\",True) \\\n",
    "      .schema(schema) \\\n",
    "      .load(\"/home/data_000019.txt\")\n",
    "\n",
    "df = df.where(col(\"HEAD\")==2)\n",
    "#df = df.withColumn(\"ABSOLUTETIME\", 25*(col('ORBIT_CNT')*1e-9*3564+col('BX_COUNTER')*1e-9+(col('TDC_MEAS')*1e-9)/30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.collect()[3]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_scint = (df.filter('(FPGA=1) AND (TDC_CHANNEL == 128)')\n",
    "                        .select(['ORBIT_CNT', 'ABSOLUTETIME'])\n",
    "                        .groupBy('ORBIT_CNT')\n",
    "                        .min()\n",
    "                        .withColumnRenamed(\"min(ABSOLUTETIME)\", 'ABSOLUTETIME_SCINT')\n",
    "                        .drop('min(ORBIT_CNT)')\n",
    "                       )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = df.filter('NOT ((FPGA=1) AND (TDC_CHANNEL == 128))').join(df_scint, [\"ORBIT_CNT\"], \"inner\")\n",
    "\n",
    "\n",
    "\n",
    "#df = df.where(f.col('ORBIT_CNT').isin(list(df_scint.ORBIT_CNT)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#df = df.filter('(FPGA=1) AND (TDC_CHANNEL >= 64) AND (TDC_CHANNEL < 128)')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df=df.where(col('ABSOLUTETIME_SCINT')>0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df=df.withColumn('DRIFTIME', col('ABSOLUTETIME')-col('ABSOLUTETIME_SCINT')+95e-9-2.6e-9).where(col('DRIFTIME')>0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.show(50)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df=df.where(col('DRIFTIME')<0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def batch_proc(batch_df, epoch_id):\n",
    "    \n",
    "    hits = batch_df.count()\n",
    "    \n",
    "    if hits!=0: \n",
    "        start = time.time()\n",
    "        #Creating the ABSOLUTETIME column\n",
    "        batch_df = batch_df.withColumn(\"ABSOLUTETIME\", 25*(col('ORBIT_CNT')*1e-9*3564+col('BX_COUNTER')*1e-9+(col('TDC_MEAS')*1e-9)/30))\n",
    "        \n",
    "        #Dataframe containing only informations about scintillator events\n",
    "        batch_df_scint = (batch_df.filter('(FPGA==1) AND (TDC_CHANNEL == 128)')\n",
    "                        .select(['ORBIT_CNT', 'ABSOLUTETIME'])\n",
    "                        .groupBy('ORBIT_CNT')\n",
    "                        .min()\n",
    "                        .withColumnRenamed(\"min(ABSOLUTETIME)\", 'ABSOLUTETIME_SCINT')\n",
    "                        .drop('min(ORBIT_CNT)')\n",
    "                       )\n",
    "        #Dividing the dataframe between chambers\n",
    "        \n",
    "        batch_df_ch0 = batch_df.filter('(FPGA==0) AND (TDC_CHANNEL >= 0) AND (TDC_CHANNEL < 64)')\n",
    "        batch_df_ch1 = batch_df.filter('(FPGA==0) AND (TDC_CHANNEL >= 64) AND (TDC_CHANNEL < 128)')\n",
    "        batch_df_ch2 = batch_df.filter('(FPGA==1) AND (TDC_CHANNEL >= 0) AND (TDC_CHANNEL < 64)')\n",
    "        batch_df_ch3 = batch_df.filter('(FPGA==1) AND (TDC_CHANNEL >= 64) AND (TDC_CHANNEL < 128)')\n",
    "        \n",
    "        batch_dfs = [batch_df_ch0, batch_df_ch1, batch_df_ch2, batch_df_ch3]\n",
    "        \n",
    "        # Counting hits for each chamber\n",
    "\n",
    "        hits_ch0 = batch_df_ch0.count()\n",
    "        hits_ch1 = batch_df_ch1.count()\n",
    "        hits_ch2 = batch_df_ch2.count()\n",
    "        hits_ch3 = batch_df_ch3.count()\n",
    "\n",
    "        end = time.time()-start\n",
    "        #Total active channels histogram\n",
    "        \n",
    "        hist1 = {}\n",
    "        \n",
    "        #Channels per orbit histogram\n",
    "            \n",
    "        hist2 = {}\n",
    "        min_hist, max_hist = batch_df.agg(min(\"ORBIT_CNT\"), max(\"ORBIT_CNT\")).rdd.flatMap(lambda x: x).collect()\n",
    "        binning = list(linspace(0, 4e8, 30))\n",
    "\n",
    "        #Active channels in orbits in which the scintillator is active histogram\n",
    "        hist3 = {}\n",
    "\n",
    "        #Drifttime histogram\n",
    "        hist4 = {}\n",
    "        #Binning for driftime - arbitrario perché a volte non troviamo né max né min.. da rivedere.\n",
    "        binning_drift = list(linspace(0,800, 40))\n",
    "        \n",
    "        \n",
    "        \n",
    "        for chamber in [0,1,2,3]:\n",
    "            hist1[chamber] = {}\n",
    "            hist2[chamber] = {}\n",
    "#             hist3[chamber] = {}\n",
    "#             hist4[chamber] = {}\n",
    "\n",
    "            bins, counts = (\n",
    "                batch_dfs[chamber].select('TDC_CHANNEL')\n",
    "                .rdd.map(lambda x: x.TDC_CHANNEL)\n",
    "                .histogram(list(arange((chamber % 2)*64,(chamber % 2 +1)*64,1)))\n",
    "            )\n",
    "            \n",
    "            bins2, counts2 = (\n",
    "                batch_dfs[chamber].groupBy(\"ORBIT_CNT\",\"TDC_CHANNEL\").count()\n",
    "                .select('ORBIT_CNT')\n",
    "                .rdd.map(lambda x: x.ORBIT_CNT)\n",
    "                .histogram(binning)\n",
    "            )\n",
    "            \n",
    "#             #Filtering only useful hits (avoid scintillators) and use inner join to consider coincident events\n",
    "#             batch_dfs[chamber] = batch_dfs[chamber].filter('NOT ((FPGA==1) AND (TDC_CHANNEL == 128))')\\\n",
    "#                                                    .join(batch_df_scint, [\"ORBIT_CNT\"], \"inner\")\n",
    "                \n",
    "#             #Creating driftime and select only positive values\n",
    "#             batch_dfs[chamber] = batch_dfs[chamber].withColumn('DRIFTIME', (col('ABSOLUTETIME')-col('ABSOLUTETIME_SCINT'))*1e9+time_offset_by_chamber[chamber])\\\n",
    "#                                 .where(col('DRIFTIME')>0) #Is it possible to remove it?\n",
    "            \n",
    "#             bins3, counts3 = (\n",
    "#                 batch_dfs[chamber].select('TDC_CHANNEL')\n",
    "#                 .rdd.map(lambda x: x.TDC_CHANNEL)\n",
    "#                 .histogram(list(arange((chamber % 2)*64,(chamber % 2 +1)*64,1)))\n",
    "#             )\n",
    "\n",
    "#             bins4, counts4 = (\n",
    "#                 batch_dfs[chamber].select('DRIFTIME')\n",
    "#                 .rdd.map(lambda x: x.DRIFTIME)\n",
    "#                 .histogram(binning_drift)\n",
    "#             )\n",
    "\n",
    "            hist1[chamber]['bins'] = list(map(int,bins)) #must convert to python integers\n",
    "            hist1[chamber]['counts'] = list(map(int,counts))\n",
    "        \n",
    "            hist2[chamber]['bins'] = list(map(int,bins2))\n",
    "            hist2[chamber]['counts'] = list(map(int,counts2))\n",
    "            \n",
    "#             hist3[chamber]['bins'] = list(map(int,bins3))\n",
    "#             hist3[chamber]['counts'] = list(map(int,counts3))\n",
    "        \n",
    "#             hist4[chamber]['bins'] = list(map(int,bins4))\n",
    "#             hist4[chamber]['counts'] = list(map(int,counts4))\n",
    "\n",
    "        #Producing the results dictionary\n",
    "        result = {\n",
    "            \"hits\" : hits,\n",
    "            \"hits_per_chamber\": [hits_ch0, hits_ch1, hits_ch2, hits_ch3],\n",
    "            \"hist_1\": hist1,\n",
    "            \"hist_2\": hist2\n",
    "#             ,\n",
    "#             \"hist_3\": hist3,\n",
    "#             \"hist_4\": hist4\n",
    "        }\n",
    "        \n",
    "        #Sending the json to the producer\n",
    "        producer.send('topic_results', json.dumps(result).encode('utf-8'))\n",
    "#         producer.flush()\n",
    "        \n",
    "    else: \n",
    "        pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
