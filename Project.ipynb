{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Management and Analysis of Physics Dataset - mod.B\n",
    "\n",
    "## Final project: Streaming processing of cosmic rays using Drift Tubes detectors\n",
    "\n",
    "The goal of this project is to reproduce a real-time processing of real data collected in a particle physics detector and publish the results in a dashboard for live monitoring.\n",
    "\n",
    "### Students:\n",
    "* Conforto Filippo (2021856)\n",
    "* Domenichetti Lorenzo (2011653)\n",
    "* Faorlin Tommaso (2021857)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Streaming notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "import findspark\n",
    "import numpy as np\n",
    "from numpy import arange\n",
    "from numpy import linspace \n",
    "import pandas as pd\n",
    "from kafka import KafkaProducer\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import from_json, col, max, min\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Creating Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('/home/packages/spark-3.1.2-bin-hadoop3.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .master(\"spark://10.67.22.100:7077\")\\\n",
    "    .appName(\"MAPD Final Project session\")\\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\")\\\n",
    "    .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)\n",
    "# spark.conf.set(\"spark.sql.inMemoryColumnarStorage.compressed\", True)\n",
    "# spark.conf.set(\"spark.sql.inMemoryColumnarStorage.batchSize\",10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://10.67.22.100:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MAPD Final Project session</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb2d8c7d780>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_BOOTSTRAP_SERVERS = \"10.67.22.226:9092\" #:226 corresponds to slave 04. \n",
    "\n",
    "inputDF = spark\\\n",
    "    .readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS)\\\n",
    "    .option('subscribe', 'topic_stream')\\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "        [StructField(\"HEAD\",        IntegerType()),\n",
    "         StructField(\"FPGA\",         IntegerType()),\n",
    "         StructField(\"TDC_CHANNEL\",  IntegerType()),\n",
    "         StructField(\"ORBIT_CNT\",    DoubleType()), ## Double to overcome overflow problems\n",
    "         StructField(\"BX_COUNTER\",   DoubleType()), ## Double to overcome overflow problems\n",
    "         StructField(\"TDC_MEAS\",    DoubleType() )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonDF = inputDF.select(from_json(col(\"value\").alias('value').cast(\"string\"), schema).alias('value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten out the dataframe\n",
    "flatDF = jsonDF.selectExpr(\"value.HEAD\", \n",
    "                           \"value.FPGA\", \n",
    "                           \"value.TDC_CHANNEL\",\n",
    "                           \"value.ORBIT_CNT\",\n",
    "                           \"value.BX_COUNTER\",\n",
    "                           \"value.TDC_MEAS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Cleaning and dataframe creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = flatDF.where(col(\"HEAD\")==2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading to consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scintillator time offset by Chamber\n",
    "time_offset_by_chamber = {\n",
    "0: 95.0 - 1.1, # Ch 0\n",
    "1: 95.0 + 6.4, # Ch 1\n",
    "2: 95.0 + 0.5, # Ch 2\n",
    "3: 95.0 - 2.6, # Ch 3\n",
    "}\n",
    "\n",
    "binning = list(linspace(0, 4e8, 30))\n",
    "\n",
    "binning_drift = list(linspace(0,800, 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_proc(batch_df, epoch_id):\n",
    "    #Creating the ABSOLUTETIME column\n",
    "    batch_df.coalesce(100)\n",
    "    batch_df.persist()\n",
    "\n",
    "    batch_df = batch_df.withColumn(\"ABSOLUTETIME\", 25*(col('ORBIT_CNT')*1e-9*3564+col('BX_COUNTER')*1e-9+(col('TDC_MEAS')*1e-9)/30))\n",
    "    \n",
    "    #Dividing the dataframe between chambers\n",
    "    batch_df_ch0 = batch_df.filter('(FPGA==0) AND (TDC_CHANNEL >= 0) AND (TDC_CHANNEL < 64)')\n",
    "    batch_df_ch1 = batch_df.filter('(FPGA==0) AND (TDC_CHANNEL >= 64) AND (TDC_CHANNEL < 128)')\n",
    "    batch_df_ch2 = batch_df.filter('(FPGA==1) AND (TDC_CHANNEL >= 0) AND (TDC_CHANNEL < 64)')\n",
    "    batch_df_ch3 = batch_df.filter('(FPGA==1) AND (TDC_CHANNEL >= 64) AND (TDC_CHANNEL < 128)')\n",
    "    \n",
    "    \n",
    "    batch_df_ch0.coalesce(100)\n",
    "    batch_df_ch1.coalesce(100)\n",
    "    batch_df_ch2.coalesce(100)\n",
    "    batch_df_ch3.coalesce(100)\n",
    "    batch_df_ch0.persist()\n",
    "    batch_df_ch1.persist()\n",
    "    batch_df_ch2.persist()\n",
    "    batch_df_ch3.persist()    \n",
    "       \n",
    "    batch_dfs = [batch_df_ch0, batch_df_ch1, batch_df_ch2, batch_df_ch3]\n",
    "    # Counting hits for each chamber\n",
    "\n",
    "    hits_ch0 = batch_df_ch0.count()\n",
    "    hits_ch1 = batch_df_ch1.count()\n",
    "    hits_ch2 = batch_df_ch2.count()\n",
    "    hits_ch3 = batch_df_ch3.count()\n",
    "    \n",
    "    hits = hits_ch0 + hits_ch1 + hits_ch2 + hits_ch3\n",
    "    if hits!=0: \n",
    "\n",
    "        #Dataframe containing only informations about scintillator events\n",
    "        batch_df_scint = (batch_df.filter('(FPGA==1) AND (TDC_CHANNEL == 128)')\n",
    "                        .select(['ORBIT_CNT', 'ABSOLUTETIME'])\n",
    "                        .groupBy('ORBIT_CNT')\n",
    "                        .min()\n",
    "                        .withColumnRenamed(\"min(ABSOLUTETIME)\", 'ABSOLUTETIME_SCINT')\n",
    "                        .drop('min(ORBIT_CNT)')\n",
    "                       )\n",
    "#         batch_df_scint.persist()\n",
    "        \n",
    "        #Total active channels histogram\n",
    "        hist1 = {}\n",
    "        \n",
    "        #Channels per orbit histogram \n",
    "        hist2 = {}\n",
    "\n",
    "        #Active channels in orbits in which the scintillator is active histogram\n",
    "        hist3 = {}\n",
    "\n",
    "        #Drifttime histogram\n",
    "        hist4 = {}\n",
    "        #Binning for driftime - arbitrario perché a volte non troviamo né max né min.. da rivedere.\n",
    "        \n",
    "        for chamber in [0,1,2,3]:\n",
    "            hist1[chamber] = {}\n",
    "            hist2[chamber] = {}\n",
    "            hist3[chamber] = {}\n",
    "            hist4[chamber] = {}\n",
    "\n",
    "            bins, counts = (\n",
    "                batch_dfs[chamber].select('TDC_CHANNEL')\n",
    "                .rdd.map(lambda x: x.TDC_CHANNEL)\n",
    "                .histogram(list(arange((chamber % 2)*64,(chamber % 2 +1)*64,1)))\n",
    "            )\n",
    "            \n",
    "            bins2, counts2 = (\n",
    "                batch_dfs[chamber].groupBy(\"ORBIT_CNT\",\"TDC_CHANNEL\").count()\n",
    "                .select('ORBIT_CNT')\n",
    "                .rdd.map(lambda x: x.ORBIT_CNT)\n",
    "                .histogram(binning)\n",
    "            )\n",
    "            \n",
    "            #Filtering only useful hits (avoid scintillators) and use inner join to consider coincident events\n",
    "            batch_dfs[chamber] = batch_dfs[chamber].filter('NOT ((FPGA==1) AND (TDC_CHANNEL == 128))')\\\n",
    "                                                   .join(batch_df_scint, [\"ORBIT_CNT\"], \"inner\")\n",
    "            #Creating driftime and select only positive values\n",
    "            batch_dfs[chamber] = batch_dfs[chamber].withColumn('DRIFTIME', (col('ABSOLUTETIME')-col('ABSOLUTETIME_SCINT'))*1e9+time_offset_by_chamber[chamber])\\\n",
    "                                .where(col('DRIFTIME')>0) #Is it possible to remove it?\n",
    "#             batch_dfs[chamber].persist()\n",
    "            \n",
    "            bins3, counts3 = (\n",
    "                batch_dfs[chamber].select('TDC_CHANNEL')\n",
    "                .rdd.map(lambda x: x.TDC_CHANNEL)\n",
    "                .histogram(list(arange((chamber % 2)*64,(chamber % 2 +1)*64,1)))\n",
    "            )\n",
    "\n",
    "            bins4, counts4 = (\n",
    "                batch_dfs[chamber].select('DRIFTIME')\n",
    "                .rdd.map(lambda x: x.DRIFTIME)\n",
    "                .histogram(binning_drift)\n",
    "            )\n",
    "            \n",
    "            hist1[chamber]['bins'] = list(map(int,bins)) #must convert to python integers\n",
    "            hist1[chamber]['counts'] = list(map(int,counts))\n",
    "\n",
    "            hist2[chamber]['bins'] = list(map(int,bins2))\n",
    "            hist2[chamber]['counts'] = list(map(int,counts2))\n",
    "\n",
    "            hist3[chamber]['bins'] = list(map(int,bins3))\n",
    "            hist3[chamber]['counts'] = list(map(int,counts3))\n",
    "\n",
    "            hist4[chamber]['bins'] = list(map(int,bins4))\n",
    "            hist4[chamber]['counts'] = list(map(int,counts4))\n",
    "\n",
    "        #Producing the results dictionary\n",
    "        result = {\n",
    "            \"hits\" : hits,\n",
    "            \"hits_per_chamber\": [hits_ch0, hits_ch1, hits_ch2, hits_ch3],\n",
    "            \"hist_1\": hist1,\n",
    "            \"hist_2\": hist2,\n",
    "            \"hist_3\": hist3,\n",
    "            \"hist_4\": hist4\n",
    "        }\n",
    "\n",
    "        #Sending the json to the producer\n",
    "        producer.send('topic_results', json.dumps(result).encode('utf-8'))\n",
    "\n",
    "        batch_df.unpersist()\n",
    "    else: \n",
    "        pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.writeStream\\\n",
    "    .foreachBatch(batch_proc)\\\n",
    "    .trigger(processingTime='5 second')\\\n",
    "    .start()\\\n",
    "    .awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset on dataset/lecture2/dimuon\n",
    "\n",
    "schema = StructType()                          \\\n",
    "      .add(\"HEAD\",        IntegerType(), True) \\\n",
    "      .add(\"FPGA\",        IntegerType(), True) \\\n",
    "      .add(\"TDC_CHANNEL\", IntegerType(), True) \\\n",
    "      .add(\"ORBIT_CNT\",   IntegerType(), True) \\\n",
    "      .add(\"BX_COUNTER\",  IntegerType(), True) \\\n",
    "      .add(\"TDC_MEAS\",    DoubleType(),  True)\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\",True) \\\n",
    "      .schema(schema) \\\n",
    "      .load(\"/home/data_000019.txt\")\n",
    "\n",
    "df = df.where(col(\"HEAD\")==2)\n",
    "#df = df.withColumn(\"ABSOLUTETIME\", 25*(col('ORBIT_CNT')*1e-9*3564+col('BX_COUNTER')*1e-9+(col('TDC_MEAS')*1e-9)/30))\n",
    "\n",
    "#df = df.limit(40000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_scint = (df.filter('(FPGA==1) AND (TDC_CHANNEL == 128)')\n",
    "                        .select(['ORBIT_CNT', 'BX_COUNTER',\"TDC_MEAS\"])\n",
    "                        .groupBy('ORBIT_CNT')\n",
    "                        .min()\n",
    "                        .withColumnRenamed(\"min(BX_COUNTER)\", 'BX_COUNTER_SCINT')\n",
    "                        .withColumnRenamed(\"min(TDC_MEAS)\", 'TDC_MEAS_SCINT')\n",
    "                        .drop('min(ORBIT_CNT)')\n",
    "           )\n",
    "\n",
    "#Filtering only useful hits (avoid scintillators) and use inner join to consider coincident events\n",
    "df = df.filter('NOT ((FPGA==1) AND (TDC_CHANNEL == 128))')\\\n",
    "            .join(df_scint, [\"ORBIT_CNT\"], \"inner\")\n",
    "##Creating driftime and select only positive values\n",
    "\n",
    "df = df.withColumn('DRIFTIME', 25*((col('BX_COUNTER')-col('BX_COUNTER_SCINT'))+(col('TDC_MEAS')-col('TDC_MEAS_SCINT'))/30))\\\n",
    "            .where(col('DRIFTIME')>0) #Is it possible to remove it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----+-----------+----------+--------+----------------+--------------+------------------+\n",
      "|ORBIT_CNT|HEAD|FPGA|TDC_CHANNEL|BX_COUNTER|TDC_MEAS|BX_COUNTER_SCINT|TDC_MEAS_SCINT|          DRIFTIME|\n",
      "+---------+----+----+-----------+----------+--------+----------------+--------------+------------------+\n",
      "| 91559218|   2|   1|         43|      3498|    10.0|            3497|          19.0|              17.5|\n",
      "| 91559218|   2|   1|         40|      3505|    23.0|            3497|          19.0|203.33333333333331|\n",
      "| 91559218|   2|   1|         41|      3505|     9.0|            3497|          19.0|191.66666666666669|\n",
      "| 91559218|   2|   0|        101|      3499|    24.0|            3497|          19.0|54.166666666666664|\n",
      "| 91559218|   2|   0|         96|      3501|    22.0|            3497|          19.0|102.49999999999999|\n",
      "+---------+----+----+-----------+----------+--------+----------------+--------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----+-----------+----------+--------+----------------+--------------+------------------+\n",
      "|ORBIT_CNT|HEAD|FPGA|TDC_CHANNEL|BX_COUNTER|TDC_MEAS|BX_COUNTER_SCINT|TDC_MEAS_SCINT|          DRIFTIME|\n",
      "+---------+----+----+-----------+----------+--------+----------------+--------------+------------------+\n",
      "| 91559218|   2|   1|         43|      3498|    10.0|            3497|          19.0|              17.5|\n",
      "| 91559218|   2|   1|         40|      3505|    23.0|            3497|          19.0|203.33333333333331|\n",
      "| 91559218|   2|   1|         41|      3505|     9.0|            3497|          19.0|191.66666666666669|\n",
      "| 91559218|   2|   0|        101|      3499|    24.0|            3497|          19.0|54.166666666666664|\n",
      "| 91559218|   2|   0|         96|      3501|    22.0|            3497|          19.0|102.49999999999999|\n",
      "| 91559218|   2|   0|        103|      3507|    19.0|            3497|          19.0|             250.0|\n",
      "| 91589950|   2|   1|         43|       619|    29.0|             619|          28.0|0.8333333333333334|\n",
      "| 91589950|   2|   1|         41|       630|    25.0|             619|          28.0|             272.5|\n",
      "| 91589950|   2|   0|        104|       620|     1.0|             619|          28.0|2.4999999999999996|\n",
      "| 91589950|   2|   0|        105|       622|     2.0|             619|          28.0|53.333333333333336|\n",
      "| 91589950|   2|   0|        107|       621|    29.0|             619|          28.0| 50.83333333333333|\n",
      "| 91589950|   2|   0|        102|      2183|    30.0|             619|          28.0|39101.666666666664|\n",
      "| 91589950|   2|   0|        102|      2193|     8.0|             619|          28.0| 39333.33333333333|\n",
      "| 91589950|   2|   1|         44|       626|    10.0|             619|          28.0|             160.0|\n",
      "| 91589950|   2|   1|         44|      1478|    25.0|             619|          28.0|           21472.5|\n",
      "| 91594772|   2|   1|        110|      1293|    17.0|            1291|           5.0|              60.0|\n",
      "| 91594772|   2|   0|        107|      1296|     8.0|            1291|           5.0|127.49999999999999|\n",
      "| 91594772|   2|   0|        108|      1292|    28.0|            1291|           5.0|44.166666666666664|\n",
      "| 91594772|   2|   1|        111|      1291|    21.0|            1291|           5.0|13.333333333333334|\n",
      "| 91594772|   2|   1|        138|      1303|     1.0|            1291|           5.0| 296.6666666666667|\n",
      "| 91594772|   2|   1|         39|      1295|    11.0|            1291|           5.0|             105.0|\n",
      "| 91594772|   2|   1|        112|      1295|    22.0|            1291|           5.0|114.16666666666666|\n",
      "| 91594772|   2|   1|         40|      1299|     9.0|            1291|           5.0|203.33333333333331|\n",
      "| 91594772|   2|   1|        113|      1297|    27.0|            1291|           5.0|168.33333333333334|\n",
      "| 91594772|   2|   1|         37|      1301|    21.0|            1291|           5.0| 263.3333333333333|\n",
      "| 91594772|   2|   0|        109|      1293|    24.0|            1291|           5.0| 65.83333333333333|\n",
      "| 91594772|   2|   0|         34|      1302|     3.0|            1291|           5.0| 273.3333333333333|\n",
      "| 91603643|   2|   1|        106|      1562|    16.0|            1558|          12.0|103.33333333333334|\n",
      "| 91603643|   2|   1|        108|      1562|    24.0|            1558|          12.0|110.00000000000001|\n",
      "| 91603643|   2|   1|        138|      1571|     1.0|            1558|          12.0| 315.8333333333333|\n",
      "| 91603643|   2|   1|         38|      1569|     7.0|            1558|          12.0|270.83333333333337|\n",
      "| 91603643|   2|   1|        107|      1561|    25.0|            1558|          12.0| 85.83333333333334|\n",
      "| 91603643|   2|   1|        109|      1563|    11.0|            1558|          12.0|124.16666666666667|\n",
      "| 91603643|   2|   1|         39|      1568|    10.0|            1558|          12.0|248.33333333333334|\n",
      "| 91603643|   2|   0|        102|      1567|    23.0|            1558|          12.0|234.16666666666669|\n",
      "| 91603643|   2|   0|        103|      1565|    27.0|            1558|          12.0|             187.5|\n",
      "| 91603643|   2|   1|         41|      1578|     7.0|            1558|          12.0| 495.8333333333333|\n",
      "| 91617563|   2|   1|         97|      1900|     6.0|            1898|          19.0|39.166666666666664|\n",
      "| 91617563|   2|   1|         40|      1906|    10.0|            1898|          19.0|             192.5|\n",
      "| 91617563|   2|   1|         99|      1903|    27.0|            1898|          19.0|131.66666666666666|\n",
      "| 91617563|   2|   1|        138|      1910|     1.0|            1898|          19.0|             285.0|\n",
      "| 91617563|   2|   0|        118|      1900|    27.0|            1898|          19.0|56.666666666666664|\n",
      "| 91617563|   2|   0|        121|      1903|     9.0|            1898|          19.0|116.66666666666667|\n",
      "| 91617563|   2|   0|        119|      1903|    20.0|            1898|          19.0|125.83333333333333|\n",
      "| 91617563|   2|   1|         41|      1908|    23.0|            1898|          19.0|253.33333333333331|\n",
      "| 91617563|   2|   1|         98|      1906|     5.0|            1898|          19.0|188.33333333333334|\n",
      "| 91617563|   2|   0|        120|      1905|    29.0|            1898|          19.0|183.33333333333331|\n",
      "| 91631466|   2|   0|         46|      1699|    10.0|            1698|          15.0|20.833333333333336|\n",
      "| 91631466|   2|   1|         92|      1706|    12.0|            1698|          15.0|             197.5|\n",
      "| 91631466|   2|   1|         36|      1705|     3.0|            1698|          15.0|             165.0|\n",
      "+---------+----+----+-----------+----------+--------+----------------+--------------+------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
